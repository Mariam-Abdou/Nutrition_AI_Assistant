{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.46.0 tokenizers==0.20.3 accelerate>=0.33 bitsandbytes==0.43.1\n",
    "!pip install langchain==0.1.20 langchain-community==0.0.38 langchain-core==0.1.52\n",
    "!pip install sentence-transformers==3.0.1 faiss-cpu==1.8.0\n",
    "!pip install pypdf==3.17.4\n",
    "!pip install protobuf==3.20.3\n",
    "\n",
    "def verify_installation():\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "        print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        import transformers\n",
    "        print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "        \n",
    "        import langchain\n",
    "        print(f\"‚úÖ LangChain: {langchain.__version__}\")\n",
    "        \n",
    "        import sentence_transformers\n",
    "        print(f\"‚úÖ Sentence Transformers: {sentence_transformers.__version__}\")\n",
    "        \n",
    "        import faiss\n",
    "        print(f\"‚úÖ FAISS: Installed\")\n",
    "        \n",
    "        import pypdf\n",
    "        print(f\"‚úÖ PyPDF: {pypdf.__version__}\")\n",
    "        \n",
    "        print(\"\\nüéâ All packages installed correctly!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-26T00:51:48.589729Z",
     "iopub.status.busy": "2025-11-26T00:51:48.589180Z",
     "iopub.status.idle": "2025-11-26T00:51:49.235921Z",
     "shell.execute_reply": "2025-11-26T00:51:49.235188Z",
     "shell.execute_reply.started": "2025-11-26T00:51:48.589702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dietplan/diet1.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:51:49.237438Z",
     "iopub.status.busy": "2025-11-26T00:51:49.236979Z",
     "iopub.status.idle": "2025-11-26T00:52:01.475639Z",
     "shell.execute_reply": "2025-11-26T00:52:01.474837Z",
     "shell.execute_reply.started": "2025-11-26T00:51:49.237417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 00:51:53.967486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764118313.992354     534 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764118313.999983     534 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline  \n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:01.477483Z",
     "iopub.status.busy": "2025-11-26T00:52:01.476778Z",
     "iopub.status.idle": "2025-11-26T00:52:01.669449Z",
     "shell.execute_reply": "2025-11-26T00:52:01.668638Z",
     "shell.execute_reply.started": "2025-11-26T00:52:01.477458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"HF-TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:03:12.263761Z",
     "iopub.status.busy": "2025-11-26T00:03:12.262827Z",
     "iopub.status.idle": "2025-11-26T00:03:12.269893Z",
     "shell.execute_reply": "2025-11-26T00:03:12.269034Z",
     "shell.execute_reply.started": "2025-11-26T00:03:12.263726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "save_dir = \"Meta_Llama-3.1_8B_Instruct_4bit_bnb\" # \"mistral_7B_Instruct_v0.2_4bit_bnb\"\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:03:14.122974Z",
     "iopub.status.busy": "2025-11-26T00:03:14.122608Z",
     "iopub.status.idle": "2025-11-26T00:05:11.035118Z",
     "shell.execute_reply": "2025-11-26T00:05:11.034141Z",
     "shell.execute_reply.started": "2025-11-26T00:03:14.122951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84e28727c4a4f6d96a5985be4f06aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc04abc9f9a04e2098b4b34e3ff53d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fae3a3d712b4ebf9d9140f6ace35200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e220fd8c3df401a99cbd1ea8ff7d4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fce97ce3df4835ba523464b3e632c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b788bb4fdedc44d58be13c4874961cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deee96c64351426988f58badc1de30f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eef1882c2014f11994817ec6beaab9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3f4c589cfb4dd9898cfa16cd23a761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee5d626bf9f46858534668a1fe77e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae58c2c2e694b93851e6fe7c14dd28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a840375b112740148d881bc60e091ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(llm_name, quantization_config=bnb_cfg, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:05:16.144088Z",
     "iopub.status.busy": "2025-11-26T00:05:16.143277Z",
     "iopub.status.idle": "2025-11-26T00:05:31.571605Z",
     "shell.execute_reply": "2025-11-26T00:05:31.570705Z",
     "shell.execute_reply.started": "2025-11-26T00:05:16.144058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model to: Meta_Llama-3.1_8B_Instruct_4bit_bnb\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.save_pretrained(save_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(\"Saved quantized model to:\", save_dir)\n",
    "except Exception as e:\n",
    "    print(\"Save 4-bit not supported in this env:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:09.027477Z",
     "iopub.status.busy": "2025-11-26T00:52:09.026877Z",
     "iopub.status.idle": "2025-11-26T00:52:09.031203Z",
     "shell.execute_reply": "2025-11-26T00:52:09.030416Z",
     "shell.execute_reply.started": "2025-11-26T00:52:09.027449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"Meta_Llama-3.1_8B_Instruct_4bit_bnb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:09.496564Z",
     "iopub.status.busy": "2025-11-26T00:52:09.495842Z",
     "iopub.status.idle": "2025-11-26T00:52:09.500808Z",
     "shell.execute_reply": "2025-11-26T00:52:09.500056Z",
     "shell.execute_reply.started": "2025-11-26T00:52:09.496534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(path_or_id):\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path_or_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path_or_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:11.246913Z",
     "iopub.status.busy": "2025-11-26T00:52:11.246256Z",
     "iopub.status.idle": "2025-11-26T00:52:11.251365Z",
     "shell.execute_reply": "2025-11-26T00:52:11.250672Z",
     "shell.execute_reply.started": "2025-11-26T00:52:11.246884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(path_or_id):\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer with trust_remote_code\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        path_or_id, \n",
    "        use_fast=True,\n",
    "        trust_remote_code=True  # Add this\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path_or_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True  # Add this\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:11.767242Z",
     "iopub.status.busy": "2025-11-26T00:52:11.766731Z",
     "iopub.status.idle": "2025-11-26T00:52:16.383916Z",
     "shell.execute_reply": "2025-11-26T00:52:16.383250Z",
     "shell.execute_reply.started": "2025-11-26T00:52:11.767216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local 4-bit checkpoint: Meta_Llama-3.1_8B_Instruct_4bit_bnb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e713dd87ca24a12b980a3d249aece56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Path(save_dir).exists():\n",
    "    print(f\"Loading local 4-bit checkpoint: {save_dir}\")\n",
    "    tokenizer, model = load_model_and_tokenizer(save_dir)\n",
    "else:\n",
    "    print(\"Local 4-bit checkpoint not found (saving likely not supported in this env).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:16.385466Z",
     "iopub.status.busy": "2025-11-26T00:52:16.385214Z",
     "iopub.status.idle": "2025-11-26T00:52:16.391661Z",
     "shell.execute_reply": "2025-11-26T00:52:16.391036Z",
     "shell.execute_reply.started": "2025-11-26T00:52:16.385446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:16.392587Z",
     "iopub.status.busy": "2025-11-26T00:52:16.392388Z",
     "iopub.status.idle": "2025-11-26T00:52:20.085177Z",
     "shell.execute_reply": "2025-11-26T00:52:20.084383Z",
     "shell.execute_reply.started": "2025-11-26T00:52:16.392572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created!\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"/kaggle/input/dietplan/diet1.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Build embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name= \"intfloat/e5-large-v2\", # \"BAAI/bge-m3\"\n",
    ")\n",
    "\n",
    "# Build vector database\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectordb.as_retriever(k=5)\n",
    "\n",
    "# Save to folder\n",
    "vectordb.save_local(\"/kaggle/working/diet_vectorstore\")\n",
    "\n",
    "print(\"Vectorstore created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:20.087004Z",
     "iopub.status.busy": "2025-11-26T00:52:20.086386Z",
     "iopub.status.idle": "2025-11-26T00:52:20.093655Z",
     "shell.execute_reply": "2025-11-26T00:52:20.092972Z",
     "shell.execute_reply.started": "2025-11-26T00:52:20.086983Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks: 32\n",
      "Printed 5 chunks\n",
      "\n",
      "|| Chunk 0 ||\n",
      "Day 1: \n",
      "Breakfast:\n",
      "1 low-fat plain Greek yogurt (6oz)\n",
      "¬æ cup blueberries\n",
      "12 almonds or 2 tablespoons  of ground flaxseed meal \n",
      "Coffee with milk and a sugar substitute\n",
      "Lunch: \n",
      "1 piece of grilled chicken (4oz)\n",
      "1 whole grain wrap (substitute corn as a gluten-free option)\n",
      "Fill with onions, peppers, spinach, \n",
      "and ‚Öì avocado 1¬º cup of strawberries on the side\n",
      "Snack: \n",
      "1 small apple with 1 tablespoon all \n",
      "natural nut butter (peanut, almond, \n",
      "cashew, sunflower seed) \n",
      "Dinner: \n",
      "1 grilled *turkey burger (4oz)1 medium baked sweet potato \n",
      "topped with cinnamon\n",
      "1 cup spinach (saut√©ed with garlic and 1 teaspoon olive oil)\n",
      "Side salad with 1 tablespoon dressing\n",
      "* Made with 99% lean ground white meatDay 2:  \n",
      "Breakfast: \n",
      "3 egg whites and 1 whole egg, \n",
      "scrambled \n",
      "2 slices of whole grain bread\n",
      "\n",
      "|| Chunk 1 ||\n",
      "Side salad with 1 tablespoon dressing\n",
      "* Made with 99% lean ground white meatDay 2:  \n",
      "Breakfast: \n",
      "3 egg whites and 1 whole egg, \n",
      "scrambled \n",
      "2 slices of whole grain bread \n",
      "(100% whole wheat, rye, oat \n",
      "or gluten-free bread)¬Ω cup cooked spinach\n",
      "¬º cup low-fat shredded cheese\n",
      "Lunch: Quinoa Bowl\n",
      "Skinless roasted chicken, diced (3oz)\n",
      "1 cup cooked quinoa1 cup chopped tomatoes \n",
      "and carrots\n",
      "‚Öì avocado or 6 olives, diced \n",
      "Snack: \n",
      "1 *Kind\n",
      "¬Æ bar \n",
      "* Avoid bars with coconut or yogurt \n",
      "Dinner: Shrimp (4oz)\n",
      "Saut√© with garlic, olive oil, and lemon \n",
      "1 cup spaghetti squash or zucchini \n",
      "¬Ω cup herb roasted potatoesDay 3:  \n",
      "Breakfast: 1 whole grain English muffin  \n",
      "1 tablespoon peanut butter \n",
      "¬æ cup blueberries2 slices low-sodium turkey\n",
      "Lunch: Tuna Salad1 can of tuna in water, drained\n",
      "1 whole grain wrap\n",
      "\n",
      "|| Chunk 2 ||\n",
      "Breakfast: 1 whole grain English muffin  \n",
      "1 tablespoon peanut butter \n",
      "¬æ cup blueberries2 slices low-sodium turkey\n",
      "Lunch: Tuna Salad1 can of tuna in water, drained\n",
      "1 whole grain wrap\n",
      "Mix with spinach, cucumber, carrots, ¬Ω cup beans (if using \n",
      "canned, make sure to rinse)\n",
      "Dress with 1 teaspoon olive oil, vinegar, and lemon (fresh garlic \n",
      "and hot pepper optional)\n",
      "Snack: \n",
      "20 baby carrots with 2 tablespoons \n",
      "hummus or 2 tablespoons guacamole \n",
      "Dinner: Chicken and Vegetable Stir-Fry 1 cup brown rice\n",
      "1 piece of grilled chicken (4oz)\n",
      "1 cup broccoli 1 cup bell peppers \n",
      "1 tablespoon olive oilDay 4:  \n",
      "Breakfast:\n",
      "¬Ω cup oatmeal  \n",
      "(cooked in water) \n",
      "¬Ω cup cup canned or frozen peaches \n",
      "(unsweetened) \n",
      "2 hard-boiled egg whites\n",
      "Lunch: Salad\n",
      "1 - 2 cups mixed greens or spinachAdd chicken, broccoli and peppers\n",
      "\n",
      "|| Chunk 3 ||\n",
      "¬Ω cup oatmeal  \n",
      "(cooked in water) \n",
      "¬Ω cup cup canned or frozen peaches \n",
      "(unsweetened) \n",
      "2 hard-boiled egg whites\n",
      "Lunch: Salad\n",
      "1 - 2 cups mixed greens or spinachAdd chicken, broccoli and peppers \n",
      "(use leftovers from Day 3‚Äôs dinner)\n",
      "1 small piece of fruit \n",
      "Snack: \n",
      "3 cups air popped popcorn\n",
      "Dinner: Grilled Chicken Burgers \n",
      "with Onions and Peppers \n",
      "1 whole wheat bun\n",
      "*Ground chicken (4oz) Add onions and peppers\n",
      "1 cup cauliflower mash\n",
      "1 cup grilled asparagus * Make extra burger for next day‚Äôs lunch\n",
      "1 2\n",
      "\n",
      "|| Chunk 4 ||\n",
      "Day 5:  \n",
      "Breakfast: Grilled Peanut Butter and \n",
      "Strawberry Sandwich \n",
      "1 whole grain bread sandwich thin (at least 3 grams of fiber)\n",
      "1 ¬Ω tablespoons all natural nut \n",
      "butter (peanut, almond, cashew, sunflower seed)\n",
      "¬Ω cup sliced strawberries \n",
      "(or other berry variety) \n",
      "Lunch: Chicken Burger Fiesta Salad \n",
      "1 leftover chicken burger  (from Day 4‚Äôs dinner)\n",
      "2 cups lettuce \n",
      "Add ¬Ω cup black beans, (if using canned, make sure to rinse) \n",
      "¬º cup low-fat cheddar cheese, and \n",
      "‚Öì avocado1 tablespoon *salsa\n",
      "Dress with 1 tablespoon lemon \n",
      "and vinegar  * Avoid salsa if you have high blood \n",
      "pressure, as it can be high in sodium\n",
      "Snack:\n",
      "1¬Ω cups edamame in shell \n",
      "(sprinkle with sea salt) \n",
      "Dinner: Egg White, Veggie Omelet \n",
      "3 egg whites and 1 whole eggAdd veggies of your choice\n",
      "2 slices of whole grain bread\n"
     ]
    }
   ],
   "source": [
    "# Get the internal document IDs\n",
    "doc_store_ids = vectordb.index_to_docstore_id.values()\n",
    "print(f\"Total number of text chunks: {len(doc_store_ids)}\")\n",
    "\n",
    "# Print the content for the first 5 documents for a quick look\n",
    "limit = 5 \n",
    "print(f\"Printed {limit} chunks\")\n",
    "for i, doc_id in enumerate(doc_store_ids):\n",
    "    if i >= limit:\n",
    "        break\n",
    "    \n",
    "    # Retrieve the actual Document object (which contains the text and metadata)\n",
    "    doc = vectordb.docstore._dict.get(doc_id)\n",
    "    \n",
    "    print(f\"\\n|| Chunk {i} ||\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:20.094782Z",
     "iopub.status.busy": "2025-11-26T00:52:20.094499Z",
     "iopub.status.idle": "2025-11-26T00:52:20.115657Z",
     "shell.execute_reply": "2025-11-26T00:52:20.114895Z",
     "shell.execute_reply.started": "2025-11-26T00:52:20.094756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def smart_router(query):\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # ===== PLAN INDICATORS =====\n",
    "    plan_keywords = [\n",
    "        # Days\n",
    "        'day ', 'week ',\n",
    "        'today', 'tomorrow',\n",
    "        # Meals\n",
    "        'breakfast', 'lunch', 'dinner', 'snack',\n",
    "        # Plan references\n",
    "        'my plan', 'the plan', 'diet plan', 'in the plan', 'from the plan',\n",
    "        'schedule', 'scheduled'\n",
    "    ]\n",
    "    \n",
    "    if any(keyword in query_lower for keyword in plan_keywords):\n",
    "        print(\"üîÑ Router: Plan Reference\")\n",
    "        return \"DOCUMENT\"\n",
    "    \n",
    "    # ===== GENERAL KNOWLEDGE INDICATORS =====\n",
    "    general_indicators = [\n",
    "        # Recipe requests\n",
    "        'how to cook', 'how to make', 'how to prepare', 'recipe for',\n",
    "        # General nutrition\n",
    "        'benefits of', 'what is', 'why is', 'should i', 'is it good',\n",
    "        # Health advice\n",
    "        'healthy', 'nutrition', 'calories in', 'protein in'\n",
    "    ]\n",
    "    \n",
    "    if any(indicator in query_lower for indicator in general_indicators):\n",
    "        print(\"üîÑ Router: General knowledge\")\n",
    "        return \"GENERAL\"\n",
    "    \n",
    "    print(\"üîÑ Router: Default\")\n",
    "    return \"DEFAULT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:21.426869Z",
     "iopub.status.busy": "2025-11-26T00:52:21.425932Z",
     "iopub.status.idle": "2025-11-26T00:52:21.431224Z",
     "shell.execute_reply": "2025-11-26T00:52:21.430325Z",
     "shell.execute_reply.started": "2025-11-26T00:52:21.426838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "document_response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"The main answer to the user's question\"),\n",
    "    ResponseSchema(name=\"items\", description=\"List of food items with portions if applicable\", type=\"list\"),\n",
    "    ResponseSchema(name=\"source\", description=\"Where this information came from in the plan\"),\n",
    "    ResponseSchema(name=\"confidence\", description=\"How confident you are in this answer\", type=\"string\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:22.191472Z",
     "iopub.status.busy": "2025-11-26T00:52:22.190776Z",
     "iopub.status.idle": "2025-11-26T00:52:22.195058Z",
     "shell.execute_reply": "2025-11-26T00:52:22.194363Z",
     "shell.execute_reply.started": "2025-11-26T00:52:22.191443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "document_parser = StructuredOutputParser.from_response_schemas(document_response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:22.553141Z",
     "iopub.status.busy": "2025-11-26T00:52:22.552369Z",
     "iopub.status.idle": "2025-11-26T00:52:22.557110Z",
     "shell.execute_reply": "2025-11-26T00:52:22.556307Z",
     "shell.execute_reply.started": "2025-11-26T00:52:22.553104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "general_response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"The main answer to the user's question\"),\n",
    "    ResponseSchema(name=\"key_points\", description=\"Key points as bullet points\", type=\"list\"),\n",
    "    ResponseSchema(name=\"notes\", description=\"Any important notes or disclaimers\", type=\"string\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:23.110534Z",
     "iopub.status.busy": "2025-11-26T00:52:23.109727Z",
     "iopub.status.idle": "2025-11-26T00:52:23.114690Z",
     "shell.execute_reply": "2025-11-26T00:52:23.113960Z",
     "shell.execute_reply.started": "2025-11-26T00:52:23.110507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "general_parser = StructuredOutputParser.from_response_schemas(general_response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:23.917590Z",
     "iopub.status.busy": "2025-11-26T00:52:23.916906Z",
     "iopub.status.idle": "2025-11-26T00:52:23.921765Z",
     "shell.execute_reply": "2025-11-26T00:52:23.921027Z",
     "shell.execute_reply.started": "2025-11-26T00:52:23.917552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful diet plan assistant. Answer the user's question using ONLY the provided diet plan context.\n",
    "\n",
    "IMPORTANT:\n",
    "- Extract exact information from the context\n",
    "- Be specific about portions and quantities\n",
    "- If information is not found, set confidence to \"low\" and explain what's missing\n",
    "- ALWAYS respond with ONLY a JSON object in this exact format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\t\"answer\": \"your main answer here\",\n",
    "\t\"items\": [\"list\", \"of\", \"food\", \"items\"],\n",
    "\t\"source\": \"where this came from in the plan\", \n",
    "\t\"confidence\": \"high/medium/low\"\n",
    "}}\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Question: {query}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:24.307592Z",
     "iopub.status.busy": "2025-11-26T00:52:24.306809Z",
     "iopub.status.idle": "2025-11-26T00:52:24.311595Z",
     "shell.execute_reply": "2025-11-26T00:52:24.310697Z",
     "shell.execute_reply.started": "2025-11-26T00:52:24.307562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GENERAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful nutrition expert. Provide clear, structured answers to general nutrition questions.\n",
    "\n",
    "ALWAYS respond with ONLY a JSON object in this exact format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\t\"answer\": \"your main answer here\",\n",
    "\t\"key_points\": [\"point 1\", \"point 2\", \"point 3\"],\n",
    "\t\"notes\": \"any important notes\"\n",
    "}}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Question: {query}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:24.663343Z",
     "iopub.status.busy": "2025-11-26T00:52:24.662806Z",
     "iopub.status.idle": "2025-11-26T00:52:24.667559Z",
     "shell.execute_reply": "2025-11-26T00:52:24.666743Z",
     "shell.execute_reply.started": "2025-11-26T00:52:24.663311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful diet plan assistant. Try to answer using the diet plan context first. \n",
    "If the information is not in the plan, use your general nutrition knowledge.\n",
    "\n",
    "STRATEGY:\n",
    "1. FIRST check if the query can be answered from the diet plan context\n",
    "2. If found in plan: provide specific details with portions\n",
    "3. If NOT in plan: use your general nutrition knowledge to provide helpful advice\n",
    "4. ALWAYS respond with ONLY a JSON object in this exact format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\t\"answer\": \"your main answer here\",\n",
    "\t\"items\": [\"list\", \"of\", \"food\", \"items\"],\n",
    "\t\"source\": \"where this came from (plan or general knowledge)\", \n",
    "\t\"confidence\": \"high/medium/low\"\n",
    "}}\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Question: {query}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:25.040589Z",
     "iopub.status.busy": "2025-11-26T00:52:25.039953Z",
     "iopub.status.idle": "2025-11-26T00:52:25.046829Z",
     "shell.execute_reply": "2025-11-26T00:52:25.045744Z",
     "shell.execute_reply.started": "2025-11-26T00:52:25.040556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_general_response(parsed_response):\n",
    "    output = [f\"üí° {parsed_response['answer']}\"]\n",
    "    \n",
    "    if parsed_response.get(\"key_points\"):\n",
    "        output.append(\"\\nüåü Key Points:\")\n",
    "        for point in parsed_response[\"key_points\"]:\n",
    "            output.append(f\"  ‚Ä¢ {point}\")\n",
    "    \n",
    "    if parsed_response.get(\"notes\"):\n",
    "        output.append(f\"\\nüìù Note: {parsed_response['notes']}\")\n",
    "    \n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:26.282736Z",
     "iopub.status.busy": "2025-11-26T00:52:26.282406Z",
     "iopub.status.idle": "2025-11-26T00:52:26.289127Z",
     "shell.execute_reply": "2025-11-26T00:52:26.288230Z",
     "shell.execute_reply.started": "2025-11-26T00:52:26.282707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_json_from_response(full_response):\n",
    "    # Look for JSON code blocks\n",
    "    json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(json_pattern, full_response, re.DOTALL)\n",
    "    \n",
    "    if matches:\n",
    "        # Take the first JSON code block found\n",
    "        json_str = matches[0].strip()\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON is malformed, try to extract just the JSON object\n",
    "            json_obj_pattern = r'\\{.*\\}'\n",
    "            obj_match = re.search(json_obj_pattern, full_response, re.DOTALL)\n",
    "            if obj_match:\n",
    "                try:\n",
    "                    return json.loads(obj_match.group())\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Fallback: try to find any JSON-like structure\n",
    "    try:\n",
    "        start_idx = full_response.find('{')\n",
    "        end_idx = full_response.rfind('}') + 1\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            json_str = full_response[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise ValueError(\"Could not extract valid JSON from response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:26.671532Z",
     "iopub.status.busy": "2025-11-26T00:52:26.670645Z",
     "iopub.status.idle": "2025-11-26T00:52:26.676726Z",
     "shell.execute_reply": "2025-11-26T00:52:26.675930Z",
     "shell.execute_reply.started": "2025-11-26T00:52:26.671494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_parsed_chain(prompt, parser):\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    def run_with_parsing(inputs):\n",
    "        raw_response = chain.run(inputs)\n",
    "        print(f\"üì® Raw response preview: {raw_response[:200]}...\")  # Debug\n",
    "        \n",
    "        try:\n",
    "            parsed_data = extract_json_from_response(raw_response)\n",
    "            return parsed_data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Parsing failed, using fallback: {e}\")\n",
    "            # Fallback: return simple structure\n",
    "            return {\n",
    "                \"answer\": raw_response.split('<|start_header_id|>assistant<|end_header_id|>')[-1].strip(),\n",
    "                \"error\": \"Automatic parsing failed\"\n",
    "            }\n",
    "    \n",
    "    return run_with_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:27.498587Z",
     "iopub.status.busy": "2025-11-26T00:52:27.497738Z",
     "iopub.status.idle": "2025-11-26T00:52:27.616933Z",
     "shell.execute_reply": "2025-11-26T00:52:27.616126Z",
     "shell.execute_reply.started": "2025-11-26T00:52:27.498554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "document_chain = create_parsed_chain(DOCUMENT_PROMPT, document_parser)\n",
    "general_chain = create_parsed_chain(GENERAL_PROMPT, general_parser)\n",
    "default_chain = create_parsed_chain(DEFAULT_PROMPT, [\"answer\", \"items\", \"source\", \"confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:29.435512Z",
     "iopub.status.busy": "2025-11-26T00:52:29.434759Z",
     "iopub.status.idle": "2025-11-26T00:52:29.441826Z",
     "shell.execute_reply": "2025-11-26T00:52:29.441053Z",
     "shell.execute_reply.started": "2025-11-26T00:52:29.435486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_document_response(parsed_response):\n",
    "    answer = parsed_response.get(\"answer\", \"No answer provided\")\n",
    "    \n",
    "    # If answer already contains the full response, just return it cleaned up\n",
    "    if \"```json\" in answer:\n",
    "        # Extract the actual answer text\n",
    "        lines = answer.split('\\n')\n",
    "        clean_lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "        answer = '\\n'.join(clean_lines).strip()\n",
    "    \n",
    "    return f\"üìã {answer}\"\n",
    "\n",
    "def format_general_response(parsed_response):\n",
    "    answer = parsed_response.get(\"answer\", \"No answer provided\")\n",
    "    \n",
    "    # If answer already contains the full response, clean it up\n",
    "    if \"```json\" in answer:\n",
    "        lines = answer.split('\\n')\n",
    "        clean_lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "        answer = '\\n'.join(clean_lines).strip()\n",
    "    \n",
    "    return f\"üí° {answer}\"\n",
    "\n",
    "def format_default_response(parsed_response):\n",
    "    answer = parsed_response.get(\"answer\", \"No answer provided\")\n",
    "    \n",
    "    # If answer already contains the full response, just return it cleaned up\n",
    "    if \"```json\" in answer:\n",
    "        # Extract the actual answer text\n",
    "        lines = answer.split('\\n')\n",
    "        clean_lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "        answer = '\\n'.join(clean_lines).strip()\n",
    "    \n",
    "    return f\"üîç {answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:30.272680Z",
     "iopub.status.busy": "2025-11-26T00:52:30.271862Z",
     "iopub.status.idle": "2025-11-26T00:52:30.279114Z",
     "shell.execute_reply": "2025-11-26T00:52:30.278418Z",
     "shell.execute_reply.started": "2025-11-26T00:52:30.272643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def answer_question(query):    \n",
    "    # Determine mode\n",
    "    mode = smart_router(query)\n",
    "    \n",
    "    try:\n",
    "        if mode == \"DOCUMENT\":\n",
    "            # Retrieve relevant context\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            print(f\"üìÑ Found {len(docs)} relevant sections\")\n",
    "            \n",
    "            # Get parsed response\n",
    "            parsed_response = document_chain({\n",
    "                \"context\": context_text, \n",
    "                \"query\": query\n",
    "            })\n",
    "            \n",
    "            # Format for user\n",
    "            return format_document_response(parsed_response)\n",
    "            \n",
    "        elif mode == \"GENERAL\":\n",
    "            print(\"üß† Using general knowledge\")\n",
    "            \n",
    "            # Get parsed response\n",
    "            parsed_response = general_chain({\n",
    "                \"query\": query\n",
    "            })\n",
    "            \n",
    "            # Format for user\n",
    "            return format_general_response(parsed_response)\n",
    "\n",
    "        else:  # DEFAULT mode\n",
    "            print(\"üîÑ Default mode\")\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            print(f\"üìÑ Found {len(docs)} relevant sections to check\")\n",
    "            \n",
    "            # Use DEFAULT chain which will try plan first, then general knowledge\n",
    "            parsed_response = default_chain({\n",
    "                \"context\": context_text, \n",
    "                \"query\": query\n",
    "            })\n",
    "            \n",
    "            # Format for user with special DEFAULT formatting\n",
    "            return format_default_response(parsed_response)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Sorry, I encountered an error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:10:48.937952Z",
     "iopub.status.busy": "2025-11-26T00:10:48.937305Z",
     "iopub.status.idle": "2025-11-26T00:11:51.809609Z",
     "shell.execute_reply": "2025-11-26T00:11:51.808922Z",
     "shell.execute_reply.started": "2025-11-26T00:10:48.937923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: what I have to eat in day 3 dinner?\n",
      "üîÑ Router: Plan Reference\n",
      "üìÑ Found 4 relevant sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Answer the user's question using ONLY the provided diet plan context.\n",
      "\n",
      "IMPORTANT:\n",
      "- Extract exact inf...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "üìã {\n",
      "  \"answer\": \"1 grilled *turkey burger (4oz), 1 medium baked sweet potato topped with cinnamon, 1 cup spinach (saut√©ed with garlic and 1 teaspoon olive oil), Side salad with 1 tablespoon dressing\",\n",
      "  \"items\": [\"grilled *turkey burger\", \"1 medium baked sweet potato\", \"1 cup spinach\", \"Side salad with 1 tablespoon dressing\"],\n",
      "  \"source\": \"Day 3:  Breakfast: 1 whole grain English muffin 1 tablespoon peanut butter ¬æ cup blueberries2 slices low-sodium turkey Lunch: Tuna Salad1 can of tuna in water, drained 1 whole grain wrap\",\n",
      "  \"confidence\": \"high\"\n",
      "}\n",
      "---\n",
      "\n",
      "üß™ Testing: what are the benefits of blueberries?\n",
      "üîÑ Router: General knowledge\n",
      "üß† Using general knowledge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful nutrition expert. Provide clear, structured answers to general nutrition questions.\n",
      "\n",
      "ALWAYS respond with ONLY a JSON objec...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "üí° {\n",
      "\t\"answer\": \"Blueberries are a nutrient-rich food that provides several health benefits due to their high content of antioxidants, vitamins, and minerals. Some of the key benefits of blueberries include:\",\n",
      "\t\"key_points\": [\n",
      "\t\t\"Rich in antioxidants, which can help protect against cell damage and reduce the risk of chronic diseases such as heart disease, cancer, and cognitive decline\",\n",
      "\t\t\"High in fiber, which can help promote digestive health and support healthy blood sugar levels\",\n",
      "\t\t\"Good source of vitamins C and K, as well as manganese and copper, which are essential for healthy bones and immune function\",\n",
      "\t\t\"May help improve memory and cognitive function due to their high content of flavonoids and anthocyanins\",\n",
      "\t\t\"May have anti-inflammatory properties, which can help reduce the risk of chronic diseases such as arthritis and other inflammatory conditions\"\n",
      "\t],\n",
      "\t\"notes\": \"Blueberries are also low in calories and high in water content, making them a nutritious and refreshing snack option.\"\n",
      "}\n",
      "---\n",
      "\n",
      "üß™ Testing: how to make healthy cheesecake?\n",
      "üîÑ Router: General knowledge\n",
      "üß† Using general knowledge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful nutrition expert. Provide clear, structured answers to general nutrition questions.\n",
      "\n",
      "ALWAYS respond with ONLY a JSON objec...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "üí° {\n",
      "\t\"answer\": \"To make a healthy cheesecake, consider the following steps: Use a whole-grain crust made from almond flour or whole-wheat pastry flour, choose a lower-fat cream cheese and a non-fat plain Greek yogurt to reduce saturated fat and calories. Add in some protein-rich Greek yogurt to increase the protein content. Use natural sweeteners like honey or maple syrup instead of refined sugar. Finally, load up on fresh fruits like berries or citrus zest to add flavor and antioxidants.\",\n",
      "\t\"key_points\": [\n",
      "\t\t\"Use a whole-grain crust\",\n",
      "\t\t\"Choose lower-fat cream cheese and non-fat plain Greek yogurt\",\n",
      "\t\t\"Add protein-rich Greek yogurt and natural sweeteners\",\n",
      "\t\t\"Load up on fresh fruits\"\n",
      "\t],\n",
      "\t\"notes\": \"Be mindful of portion sizes and consider using a sugar-free sweetener if you're watching your sugar intake.\"\n",
      "}\n",
      "---\n",
      "\n",
      "üß™ Testing: can I replace rice with quinoa in my plan?\n",
      "üîÑ Router: Plan Reference\n",
      "üìÑ Found 4 relevant sections\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Answer the user's question using ONLY the provided diet plan context.\n",
      "\n",
      "IMPORTANT:\n",
      "- Extract exact inf...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "üìã {\n",
      "  \"answer\": \"Yes, you can replace rice with quinoa in your plan, as quinoa is already included in the meal plan.\",\n",
      "  \"items\": [\"quinoa\"],\n",
      "  \"source\": \"Breakfast: Quinoa Bowl (mix together) and Dinner: Shrimp and Veggie Stir-Fry and Dinner: Turkey Meatballs with Quinoa and Roasted Kale and Breakfast: Pumpkin Quinoa\",\n",
      "  \"confidence\": \"high\"\n",
      "}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    test_questions = [\n",
    "        \"what I have to eat in day 3 dinner?\",\n",
    "        \"what are the benefits of blueberries?\",\n",
    "        \"how to make healthy cheesecake?\",\n",
    "        \"can I replace rice with quinoa in my plan?\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nüß™ Testing: {question}\")\n",
    "        try:\n",
    "            result = answer_question(question)\n",
    "            print(result)\n",
    "            print(\"---\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"---\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ü§ñ Smart Diet Assistant Ready!\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nüí¨ Ask a question: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            answer = answer_question(user_query)\n",
    "            print(f\"\\n{answer}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:16:36.364121Z",
     "iopub.status.busy": "2025-11-26T00:16:36.363768Z",
     "iopub.status.idle": "2025-11-26T00:16:39.831381Z",
     "shell.execute_reply": "2025-11-26T00:16:39.830337Z",
     "shell.execute_reply.started": "2025-11-26T00:16:36.364099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn pyngrok nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-26T00:51:07.798Z",
     "iopub.execute_input": "2025-11-26T00:51:04.530569Z",
     "iopub.status.busy": "2025-11-26T00:51:04.530234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Stopping old server...\n"
     ]
    }
   ],
   "source": [
    "# Restart server\n",
    "import os\n",
    "import signal\n",
    "print(\"üî¥ Stopping old server...\")\n",
    "os.system(\"kill -9 $(lsof -t -i:8000) 2>/dev/null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T00:52:48.431498Z",
     "iopub.status.busy": "2025-11-26T00:52:48.430852Z",
     "iopub.status.idle": "2025-11-26T00:56:45.412045Z",
     "shell.execute_reply": "2025-11-26T00:56:45.411187Z",
     "shell.execute_reply.started": "2025-11-26T00:52:48.431469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuring ngrok tunnel...\n",
      "\n",
      "======================================================================\n",
      "üöÄ PUBLIC URL: NgrokTunnel: \"https://trophoplasmic-removed-cecily.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
      "======================================================================\n",
      "\n",
      "üìö API Docs: NgrokTunnel: \"https://trophoplasmic-removed-cecily.ngrok-free.dev\" -> \"http://localhost:8000\"/docs\n",
      "üìñ ReDoc: NgrokTunnel: \"https://trophoplasmic-removed-cecily.ngrok-free.dev\" -> \"http://localhost:8000\"/redoc\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Server is running!\n",
      "\n",
      "üîó Save this URL for your Streamlit app:\n",
      "   API_URL = 'NgrokTunnel: \"https://trophoplasmic-removed-cecily.ngrok-free.dev\" -> \"http://localhost:8000\"'\n",
      "\n",
      "üìù Example Python request:\n",
      "\n",
      "import requests\n",
      "\n",
      "response = requests.post(\n",
      "    'NgrokTunnel: \"https://trophoplasmic-removed-cecily.ngrok-free.dev\" -> \"http://localhost:8000\"/ask',\n",
      "    json={'query': 'What should I eat for day 1 breakfast?'}\n",
      ")\n",
      "print(response.json())\n",
      "\n",
      "\n",
      "‚è≥ Server running... Press Ctrl+C to stop\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [534]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Default\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Try to answer using the diet plan context first. \n",
      "If the information is not in the plan, use your gen...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Plan Reference\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Answer the user's question using ONLY the provided diet plan context.\n",
      "\n",
      "IMPORTANT:\n",
      "- Extract exact inf...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Default\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Try to answer using the diet plan context first. \n",
      "If the information is not in the plan, use your gen...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Plan Reference\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Answer the user's question using ONLY the provided diet plan context.\n",
      "\n",
      "IMPORTANT:\n",
      "- Extract exact inf...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Default\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Try to answer using the diet plan context first. \n",
      "If the information is not in the plan, use your gen...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: General knowledge\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful nutrition expert. Provide clear, structured answers to general nutrition questions.\n",
      "\n",
      "ALWAYS respond with ONLY a JSON objec...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Router: Default\n",
      "üì® Raw response preview: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful diet plan assistant. Try to answer using the diet plan context first. \n",
      "If the information is not in the plan, use your gen...\n",
      "‚ö†Ô∏è Parsing failed, using fallback: Could not extract valid JSON from response\n",
      "INFO:     41.45.62.179:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "\n",
      "üõë Server stopped\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import uvicorn\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "NGROK_AUTH_TOKEN = \"NGROK-TOKEN\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def clean_json_response(raw_response):\n",
    "    try:\n",
    "        # Step 1: Get the outer structure\n",
    "        if isinstance(raw_response, dict):\n",
    "            outer_data = raw_response\n",
    "        else:\n",
    "            outer_data = extract_json_from_response(str(raw_response))\n",
    "        \n",
    "        # Step 2: Get the answer field\n",
    "        answer_field = outer_data.get(\"answer\", \"\")\n",
    "        \n",
    "        # Step 3: Check if answer itself is JSON (nested structure)\n",
    "        if isinstance(answer_field, str) and answer_field.strip().startswith('{'):\n",
    "            try:\n",
    "                # Parse the nested JSON\n",
    "                inner_json = json.loads(answer_field)\n",
    "                \n",
    "                # Extract all fields from the nested JSON\n",
    "                return {\n",
    "                    \"answer\": inner_json.get(\"answer\", \"\"),\n",
    "                    \"items\": inner_json.get(\"items\", []),\n",
    "                    \"source\": inner_json.get(\"source\", \"\"),\n",
    "                    \"confidence\": inner_json.get(\"confidence\", \"medium\"),\n",
    "                    \"key_points\": inner_json.get(\"key_points\", []),\n",
    "                    \"notes\": inner_json.get(\"notes\", \"\")\n",
    "                }\n",
    "            except json.JSONDecodeError:\n",
    "                # If parsing fails, use the string as-is\n",
    "                pass\n",
    "        \n",
    "        # Step 4: If not nested, use outer structure\n",
    "        return {\n",
    "            \"answer\": answer_field,\n",
    "            \"items\": outer_data.get(\"items\", []),\n",
    "            \"source\": outer_data.get(\"source\", \"\"),\n",
    "            \"confidence\": outer_data.get(\"confidence\", \"medium\"),\n",
    "            \"key_points\": outer_data.get(\"key_points\", []),\n",
    "            \"notes\": outer_data.get(\"notes\", \"\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning response: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Fallback\n",
    "        if isinstance(raw_response, dict):\n",
    "            answer = raw_response.get(\"answer\", str(raw_response))\n",
    "            # Try one more time to parse if it's JSON string\n",
    "            if isinstance(answer, str) and answer.strip().startswith('{'):\n",
    "                try:\n",
    "                    nested = json.loads(answer)\n",
    "                    return {\n",
    "                        \"answer\": nested.get(\"answer\", answer),\n",
    "                        \"items\": nested.get(\"items\", []),\n",
    "                        \"source\": nested.get(\"source\", \"\"),\n",
    "                        \"confidence\": nested.get(\"confidence\", \"low\")\n",
    "                    }\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"items\": [],\n",
    "                \"source\": \"model\",\n",
    "                \"confidence\": \"low\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(raw_response),\n",
    "            \"items\": [],\n",
    "            \"source\": \"model\",\n",
    "            \"confidence\": \"low\"\n",
    "        }\n",
    "\n",
    "# ============================================\n",
    "# REQUEST/RESPONSE\n",
    "# ============================================\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    mode: str\n",
    "    items: list = []\n",
    "    source: str = \"\"\n",
    "    confidence: str = \"\"\n",
    "    key_points: list = []\n",
    "    notes: str = \"\"\n",
    "\n",
    "# ============================================\n",
    "# FASTAPI APP\n",
    "# ============================================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Smart Diet Assistant API\",\n",
    "    description=\"AI-powered diet plan assistant\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"message\": \"ü•ó Smart Diet Assistant API\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model\": \"Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"vectordb\": \"FAISS with e5-large-v2\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/ask\", response_model=QueryResponse)\n",
    "async def ask_question(request: QueryRequest):\n",
    "    \"\"\"Ask a diet-related question\"\"\"\n",
    "    try:\n",
    "        query = request.query.strip()\n",
    "        \n",
    "        if not query:\n",
    "            raise HTTPException(status_code=400, detail=\"Query cannot be empty\")\n",
    "        \n",
    "        # Use your existing answer_question function\n",
    "        mode = smart_router(query)\n",
    "        \n",
    "        if mode == \"DOCUMENT\":\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            raw_response = document_chain({\"context\": context_text, \"query\": query})\n",
    "            \n",
    "        elif mode == \"GENERAL\":\n",
    "            raw_response = general_chain({\"query\": query})\n",
    "            \n",
    "        else:  # DEFAULT\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            raw_response = default_chain({\"context\": context_text, \"query\": query})\n",
    "        \n",
    "        # Clean the response (remove JSON formatting)\n",
    "        cleaned = clean_json_response(raw_response)\n",
    "        \n",
    "        return QueryResponse(\n",
    "            answer=cleaned[\"answer\"],\n",
    "            mode=mode,\n",
    "            items=cleaned.get(\"items\", []),\n",
    "            source=cleaned.get(\"source\", \"\"),\n",
    "            confidence=cleaned.get(\"confidence\", \"medium\"),\n",
    "            key_points=cleaned.get(\"key_points\", []),\n",
    "            notes=cleaned.get(\"notes\", \"\")\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/upload-diet-plan\")\n",
    "async def upload_diet_plan(file: UploadFile = File(...)):\n",
    "    \"\"\"Upload a new diet plan PDF\"\"\"\n",
    "    try:\n",
    "        if not file.filename.lower().endswith(\".pdf\"):\n",
    "            raise HTTPException(status_code=400, detail=\"File must be a PDF\")\n",
    "        \n",
    "        pdf_bytes = await file.read()\n",
    "        temp_path = \"/kaggle/working/temp_diet.pdf\"\n",
    "        \n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            f.write(pdf_bytes)\n",
    "        \n",
    "        # Process the PDF\n",
    "        loader = PyPDFLoader(temp_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        \n",
    "        # Update global vectordb\n",
    "        global vectordb, retriever\n",
    "        vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectordb.as_retriever(k=5)\n",
    "        vectordb.save_local(\"/kaggle/working/diet_vectorstore\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"chunks\": len(chunks),\n",
    "            \"filename\": file.filename\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ============================================\n",
    "# START SERVER\n",
    "# ============================================\n",
    "\n",
    "print(\"Configuring ngrok tunnel...\")\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ PUBLIC URL: {public_url}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nAPI Docs: {public_url}/docs\")\n",
    "print(f\"ReDoc: {public_url}/redoc\")\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Apply nest_asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Start server in background\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"‚úÖ Server is running!\")\n",
    "print(\"\\nUse this URL for your Streamlit app:\")\n",
    "print(f\"   API_URL = '{public_url}'\")\n",
    "print()\n",
    "\n",
    "# Keep server alive\n",
    "try:\n",
    "    print(\"\\nServer running...\")\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Server stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8834173,
     "sourceId": 13865869,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
